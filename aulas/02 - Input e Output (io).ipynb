{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "CUxAWNz4HKrc",
   "metadata": {
    "id": "CUxAWNz4HKrc"
   },
   "source": [
    "# Input/Output no PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1nOybPXHQxA",
   "metadata": {
    "id": "t1nOybPXHQxA"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d649d9",
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1770032884640,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "29d649d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/03 22:32:54 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
      "26/02/03 22:32:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/03 22:32:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/03 22:32:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GRiquq2ELjfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21126,
     "status": "ok",
     "timestamp": 1770032920563,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "GRiquq2ELjfP",
    "outputId": "a1897f7d-a307-48c9-99dd-9643a078c3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P7yFQ9LxHW90",
   "metadata": {
    "id": "P7yFQ9LxHW90"
   },
   "source": [
    "## Lendo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84776ca8",
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1770033161629,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "84776ca8"
   },
   "outputs": [],
   "source": [
    "# Vamos usar um dataset menor para testar todas as configurações\n",
    "# No fim iremos aplicar todos os conhecimentos adquiridos em um dataset da Olist\n",
    "playground_path = \"data/playground/notas.csv\"\n",
    "\n",
    "df = spark.read.csv(path=playground_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23b9b9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1770033162697,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "f23b9b9d",
    "outputId": "599f2cd6-0065-4efc-9a2f-360d15a12da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|nome;nascimento;nota|\n",
      "|  João;20/05/1998;10|\n",
      "|  Ana;19/05/2000;9.5|\n",
      "| Maria;01/04/1986;-1|\n",
      "|Fabiana;20/05/1973;6|\n",
      "|   Jose;16/08/2005;9|\n",
      "|  Vitor;20/05/1996;8|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4IHLac0IwVJ",
   "metadata": {
    "id": "t4IHLac0IwVJ"
   },
   "source": [
    "### Parametro de leitura "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f061c107",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1770033163816,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "f061c107",
    "outputId": "85fef1ea-0851-472a-ce06-cbf7b80301e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+\n",
      "|   nome|nascimento|nota|\n",
      "+-------+----------+----+\n",
      "|   João|20/05/1998|  10|\n",
      "|    Ana|19/05/2000| 9.5|\n",
      "|  Maria|01/04/1986|  -1|\n",
      "|Fabiana|20/05/1973|   6|\n",
      "|   Jose|16/08/2005|   9|\n",
      "|  Vitor|20/05/1996|   8|\n",
      "+-------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definindo que a primeira linha do CSV é o header e o separador é ;\n",
    "spark.read.csv(playground_path, header=True, sep=';').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55008592",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1770033164350,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "55008592",
    "outputId": "3ea90eca-d9ba-4ce3-8c72-4951f0470e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+\n",
      "|   nome|nascimento|nota|\n",
      "+-------+----------+----+\n",
      "|   João|20/05/1998|  10|\n",
      "|    Ana|19/05/2000| 9.5|\n",
      "|  Maria|01/04/1986|NULL|\n",
      "|Fabiana|20/05/1973|   6|\n",
      "|   Jose|16/08/2005|   9|\n",
      "|  Vitor|20/05/1996|   8|\n",
      "+-------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Também podemos definir se um caracter é a representação de null\n",
    "\n",
    "df = spark.read.csv(\n",
    "    path=playground_path,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    nullValue=\"-1\",\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fa9f6be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1770033164370,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "7fa9f6be",
    "outputId": "abefd314-dc19-4d7d-92bd-16d1e90aa955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nascimento: string (nullable = true)\n",
      " |-- nota: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mas o tipo dos dados estão corretos?\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "466e0384",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1770033165373,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "466e0384",
    "outputId": "1c755260-9337-4add-e632-215d840c09c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nascimento: string (nullable = true)\n",
      " |-- nota: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nascimento: date (nullable = true)\n",
      " |-- nota: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Formas de corrigir esse problema com CSV\n",
    "\n",
    "# 1. inferSchema + Date Formate\n",
    "spark.read.csv(\n",
    "    path=playground_path,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    nullValue=\"-1\",\n",
    "    inferSchema=True,\n",
    ").printSchema()\n",
    "\n",
    "\n",
    "spark.read.csv(\n",
    "    path=playground_path,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    nullValue=\"-1\",\n",
    "    inferSchema=True,\n",
    "    dateFormat=\"dd/MM/yyyy\" #Add this line\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flURSigdNMa0",
   "metadata": {
    "id": "flURSigdNMa0"
   },
   "source": [
    "## Lendo Json\n",
    "O padrão nativo do Spark/Big Data é o JSON Lines, onde não usa vírgulas separando as linhas e sem colchetes de array envolvendo o arquivo todo. [Documentação](https://jsonlines.org/examples/)\n",
    "\n",
    "Exemplo\n",
    "```\n",
    "  {\"nome\": \"João\", \"nascimento\": \"20/05/1998\", \"nota\": 10}\n",
    "  {\"nome\": \"Ana\", \"nascimento\": \"19/05/2000\", \"nota\": 9.5}\n",
    "```\n",
    "\n",
    "Como estamos utilizando o formato padrão de json precisamos da opção `multiLine=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2Qnhiy6bNIg9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1206,
     "status": "ok",
     "timestamp": 1770033352194,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "2Qnhiy6bNIg9",
    "outputId": "a9cc57d7-0387-4181-ff1c-bfc558454973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+\n",
      "|nascimento|   nome|nota|\n",
      "+----------+-------+----+\n",
      "|20/05/1998|   João|10.0|\n",
      "|19/05/2000|    Ana| 9.5|\n",
      "|01/04/1986|  Maria|-1.0|\n",
      "|20/05/1973|Fabiana| 6.0|\n",
      "|16/08/2005|   Jose| 9.0|\n",
      "|20/05/1996|  Vitor| 8.0|\n",
      "+----------+-------+----+\n",
      "\n",
      "root\n",
      " |-- nascimento: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nota: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = \"data/playground/notas.json\"\n",
    "\n",
    "df_lines = spark.read.json(\n",
    "  json_path,\n",
    "  multiLine=True,\n",
    "  dateFormat=\"dd/MM/yyyy\"\n",
    ")\n",
    "df_lines.show()\n",
    "df_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hs-YB7nmNgt4",
   "metadata": {
    "id": "hs-YB7nmNgt4"
   },
   "source": [
    "### Especificando Schema\n",
    "\n",
    "A forma garantir que os tipos estão corretos é definir o schema manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7at47FdWM5uE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1770033437732,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "7at47FdWM5uE",
    "outputId": "eb612881-84ca-451c-8f91-191bba91e1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nascimento: date (nullable = true)\n",
      " |-- nota: double (nullable = true)\n",
      "\n",
      "+-------+----------+----+\n",
      "|   nome|nascimento|nota|\n",
      "+-------+----------+----+\n",
      "|   João|1998-05-20|10.0|\n",
      "|    Ana|2000-05-19| 9.5|\n",
      "|  Maria|1986-04-01|-1.0|\n",
      "|Fabiana|1973-05-20| 6.0|\n",
      "|   Jose|2005-08-16| 9.0|\n",
      "|  Vitor|1996-05-20| 8.0|\n",
      "+-------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    DateType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"nascimento\", DateType(), True),\n",
    "    StructField(\"nota\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_schema = spark.read.json(\n",
    "    json_path,\n",
    "    schema=schema,\n",
    "    multiLine=True,\n",
    "    dateFormat=\"dd/MM/yyyy\"\n",
    ")\n",
    "\n",
    "df_schema.printSchema()\n",
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Un_gSBU9OWx1",
   "metadata": {
    "id": "Un_gSBU9OWx1"
   },
   "source": [
    "## Read Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "p4PQbYuEOX0r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1770033732387,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "p4PQbYuEOX0r",
    "outputId": "831b54da-8ad2-41c6-82e5-b53052504813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho Parquet: 6.44 MB\n",
      "Tamanho CSV: 14.72 MB\n"
     ]
    }
   ],
   "source": [
    "# Parquet é um formato que compacta melhor os nossos dados\n",
    "\n",
    "import os\n",
    "\n",
    "file_parquet = 'data/playground/olist_order_items_dataset.parquet'\n",
    "file_csv = 'data/raw/olist_order_items_dataset.csv'\n",
    "\n",
    "size1 = os.path.getsize(file_parquet) / (1024 * 1024)\n",
    "size2 = os.path.getsize(file_csv) / (1024 * 1024)\n",
    "\n",
    "print(f\"Tamanho Parquet: {size1:.2f} MB\")\n",
    "print(f\"Tamanho CSV: {size2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "qCOOf9rRO6w5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3408,
     "status": "ok",
     "timestamp": 1770033786324,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "qCOOf9rRO6w5",
    "outputId": "9228ad6c-95dd-4ae1-af09-22aac76f89d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
      "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n",
      "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n",
      "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_path = 'data/playground/olist_order_items_dataset.parquet'\n",
    "\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yaD8GnniPNK6",
   "metadata": {
    "id": "yaD8GnniPNK6"
   },
   "source": [
    "### Multi Files\n",
    "\n",
    "Trabalhando com spark vai ser comum vcs receberem tabelas divididas em varios arquivos.\n",
    "\n",
    "Isso acontece porque o Spark é um sistema distribuído. Vários executores podem estar escrevendo partes do arquivo simultaneamente. Ao final, teremos vários arquivos `part-00000`, `part-00001`, etc., dentro da pasta de destino.\n",
    "\n",
    "Exemplo\n",
    "```\n",
    "dataset.parquet/ --pasta\n",
    " ├── part-00000.parquet --arquivos\n",
    " ├── part-00001.parquet\n",
    " └── part-00002.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ZgIAm-kIPLtv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5629,
     "status": "ok",
     "timestamp": 1770033890634,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "ZgIAm-kIPLtv",
    "outputId": "62a7449a-a791-40ad-fb56-eb19284349f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|304dfaa2530a17749...|            1|0aed9d548c366ad80...|643214e62b870443c...|2018-05-18 15:59:46|  43.8|         7.39|\n",
      "|3948c2f40e71b2178...|            1|887dba291adc295b5...|98dac6635aee4995d...|2017-05-11 16:35:11|  26.4|         14.1|\n",
      "|274954faa1775d863...|            1|96277485af1fa20c3...|b499c00f28f4b7069...|2017-12-04 12:53:20| 59.99|        15.17|\n",
      "|43ee3ed7fa79e982f...|            1|e67307ff0f15ade43...|f4aba7c0bca51484c...|2017-06-02 17:30:19| 18.99|         15.1|\n",
      "|24508eba7537515f4...|            1|29b85c2f1ec5f4d02...|44073f8b7e41514de...|2017-04-26 21:35:19| 186.9|        21.36|\n",
      "|3addeedb2235eeb1e...|            1|58bef3ad35aed45b9...|c3867b4666c7d7686...|2018-03-29 02:31:02| 125.0|        14.96|\n",
      "|434061e3b7fa3c393...|            1|368c6c730842d7801...|1f50f920176fa81da...|2017-10-05 02:30:35|  59.9|        17.67|\n",
      "|06a1cb50e0911f3b0...|            1|cc1b5f3a64b4f452b...|0daf5180aa44356f6...|2018-08-24 04:04:20|  39.9|        19.19|\n",
      "|1bed42b5193345848...|            1|a5b3765e039c48832...|4e7c18b98d84e05cb...|2017-10-10 16:04:37|  10.0|         15.1|\n",
      "|0e3737a398f635ed0...|            1|b86342c63a6cf10c0...|080199a181c46c657...|2018-01-11 13:07:18|179.49|         9.45|\n",
      "|1b900d4d10e97580e...|            1|9ead3847aad283a08...|fac1a9017ade7bdc9...|2018-05-29 18:57:02|  99.9|        22.41|\n",
      "|1d39a1c8b0608d354...|            1|723f62c36c2980377...|6560211a19b47992c...|2018-03-04 22:28:35|  65.0|        17.73|\n",
      "|3a3c98aede7994faf...|            1|e51136ef1d291cf07...|9bcdfa7b615b3abb9...|2017-02-05 21:14:52| 359.9|        17.63|\n",
      "|02ead97e330f22204...|            1|e338f548cda0b5265...|cab85505710c7cb9b...|2017-07-03 03:43:25|  54.9|        15.13|\n",
      "|1e3b30b2e066ae120...|            1|eaee6d611cea1d76f...|b335c59ab742f751a...|2017-08-07 19:23:06| 27.16|        17.92|\n",
      "|24197cf765af52797...|            1|c4c273fc9291c33a5...|620c87c171fb2a6dd...|2017-04-06 16:30:16| 299.9|        12.71|\n",
      "|0e556f5eafbf3eb39...|            2|aafec8ef0f0e7bf2f...|0ea22c1cfbdc755f8...|2016-10-10 19:31:39|  24.9|        22.93|\n",
      "|11104e15dc0997a6b...|            1|bb42f37fc3d9130e4...|da8622b14eb17ae28...|2018-04-12 03:15:32|  44.9|         5.56|\n",
      "|1add1aa4c6e709fd7...|            1|21fb5057dd6a737df...|b14db04aa7881970e...|2016-10-11 08:29:50|  49.0|        25.15|\n",
      "|39735cad3fa0e9f2e...|            1|1078e04ece7f1498a...|7040e82f899a04d1b...|2018-08-07 03:10:15|  24.9|        11.19|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Ler um arquivo ou multiplos arquivos é feito da mesma forma\n",
    "\n",
    "multi_file_path = \"data/playground/olist_order__multi_files\"\n",
    "df = spark.read.parquet(multi_file_path)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "folder_intro",
   "metadata": {
    "id": "folder_intro"
   },
   "source": [
    "## Salvando Arquivos\n",
    "\n",
    "Uma característica importante do Spark é que, ao salvar dados (seja CSV, Parquet, JSON, etc.), ele **sempre cria uma pasta** e não um arquivo único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "create_data",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4745,
     "status": "ok",
     "timestamp": 1770033974844,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "create_data",
    "outputId": "ebe9d4bf-dc63-498a-fdb4-a554d303203b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|  Nome|Idade|Estado|\n",
      "+------+-----+------+\n",
      "|  João|   25|    RJ|\n",
      "| Maria|   30|    SP|\n",
      "| Pedro|   22|    MG|\n",
      "|   Ana|   28|    RJ|\n",
      "|Carlos|   35|    SP|\n",
      "+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando um DataFrame de exemplo\n",
    "data = [\n",
    "    (\"João\", 25, \"RJ\"),\n",
    "    (\"Maria\", 30, \"SP\"),\n",
    "    (\"Pedro\", 22, \"MG\"),\n",
    "    (\"Ana\", 28, \"RJ\"),\n",
    "    (\"Carlos\", 35, \"SP\")\n",
    "]\n",
    "columns = [\"Nome\", \"Idade\", \"Estado\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv_write",
   "metadata": {
    "id": "csv_write"
   },
   "source": [
    "### Salvando em CSV\n",
    "As configurações usadas no read também podem ser utilizadas no write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_csv_example",
   "metadata": {
    "executionInfo": {
     "elapsed": 1287,
     "status": "ok",
     "timestamp": 1770034015463,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "write_csv_example"
   },
   "outputs": [],
   "source": [
    "df.write.csv(\n",
    "    \"data/playground/write/output_csv\",\n",
    "    header=True,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712df08d",
   "metadata": {},
   "source": [
    "#### Modo de escrita\n",
    "Se a gente rerodar o comando superior, ele vai retornar um erro, pq o arquivo já existe. Para resolver isso, temos que definir que podemos sobreescrever usando o modo de escrita (mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a5d69",
   "metadata": {
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1770034032839,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "063a5d69"
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\n",
    "    \"data/playground/write/output_csv\",\n",
    "    header=True,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parquet_write",
   "metadata": {
    "id": "parquet_write"
   },
   "source": [
    "### Salvando em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_parquet_example",
   "metadata": {
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1770034045132,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "write_parquet_example"
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"data/playground/write/output_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "repartition_intro",
   "metadata": {
    "id": "repartition_intro"
   },
   "source": [
    "### Reparticionamento (Repartition)\n",
    "\n",
    "Como mencionado, o Spark salva os dados em vários arquivos (partições). Podemos controlar quantos arquivos serão gerados usando `repartition()` ou `coalesce()`.\n",
    "\n",
    "- `repartition(N)`: Aumenta ou diminui o número de partições (faz um shuffle completo dos dados).\n",
    "- `coalesce(N)`: Diminui o número de partições (mais eficiente que repartition para reduzir, pois evita shuffle total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "repartition_example",
   "metadata": {
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1770034056303,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "repartition_example"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"data/playground/write/output_parquet_one_file\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa74978",
   "metadata": {
    "id": "6aa74978"
   },
   "source": [
    "## Aplicando no dataset de review da Olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbd6d881",
   "metadata": {
    "executionInfo": {
     "elapsed": 4366,
     "status": "ok",
     "timestamp": 1770033236833,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "bbd6d881"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"review_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"review_score\", IntegerType()),\n",
    "    StructField(\"review_comment_title\", StringType()),\n",
    "    StructField(\"review_comment_message\", StringType()),\n",
    "    StructField(\"review_creation_date\", TimestampType()),\n",
    "    StructField(\"review_answer_timestamp\", TimestampType()) \n",
    "])\n",
    "\n",
    "olist_path = \"data/raw/olist_order_reviews_dataset.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    path=olist_path,\n",
    "    header=True,\n",
    "    sep=',',\n",
    "    schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9af4106c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1770033236867,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "9af4106c",
    "outputId": "d83d6386-64f7-4959-c84d-79731e781a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_comment_title: string (nullable = true)\n",
      " |-- review_comment_message: string (nullable = true)\n",
      " |-- review_creation_date: timestamp (nullable = true)\n",
      " |-- review_answer_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5ed6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"data/processed/olist_order_reviews_dataset\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
