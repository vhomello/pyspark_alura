{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5.2 - Features dos últimos review\n",
                "\n",
                "Neste notebook, exploraremos outras funções Window no PySpark. <br>\n",
                "Nesse processo vamos criar features baseadas nos reviews realizados por clientes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "26/02/07 01:22:07 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
                        "26/02/07 01:22:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "26/02/07 01:22:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "26/02/07 01:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
                        "26/02/07 01:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
                        "26/02/07 01:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.window import Window\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Join datasets\n",
                "\n",
                "Como vamos olhar os reviews do customer_id, precisamos juntas os datasets\n",
                "- reviews\n",
                "- orders\n",
                "- customers\n",
                "\n",
                "Dado que já temos conhecimento de como fazer join no PySpark, vamos utilizar o seguinte código"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- review_id: string (nullable = true)\n",
                        " |-- order_id: string (nullable = true)\n",
                        " |-- review_score: string (nullable = true)\n",
                        " |-- review_comment_title: string (nullable = true)\n",
                        " |-- review_comment_message: string (nullable = true)\n",
                        " |-- review_creation_date: string (nullable = true)\n",
                        " |-- review_answer_timestamp: string (nullable = true)\n",
                        "\n",
                        "root\n",
                        " |-- order_id: string (nullable = true)\n",
                        " |-- customer_id: string (nullable = true)\n",
                        " |-- order_status: string (nullable = true)\n",
                        " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
                        " |-- order_approved_at: timestamp (nullable = true)\n",
                        " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
                        " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
                        " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
                        "\n",
                        "root\n",
                        " |-- customer_id: string (nullable = true)\n",
                        " |-- customer_unique_id: string (nullable = true)\n",
                        " |-- customer_zip_code_prefix: integer (nullable = true)\n",
                        " |-- customer_city: string (nullable = true)\n",
                        " |-- customer_state: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "review_path = \"data/raw/olist_order_reviews_dataset.csv\"\n",
                "order_path = \"data/raw/olist_orders_dataset.csv\"\n",
                "customer_path = \"data/raw/olist_customers_dataset.csv\"\n",
                "\n",
                "review_df = spark.read.csv(review_path, header=True, inferSchema=True)\n",
                "order_df = spark.read.csv(order_path, header=True, inferSchema=True)\n",
                "customer_df = spark.read.csv(customer_path, header=True, inferSchema=True)\n",
                "\n",
                "review_df.printSchema()\n",
                "order_df.printSchema()\n",
                "customer_df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "|         customer_id|            order_id|review_creation_date|review_score|\n",
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "|2a7d8740c329c52ba...|cd53e0cf7df727e89...| 2017-07-19 00:00:00|           5|\n",
                        "|bf57ac29566f5216f...|959d645c348f4b237...| 2018-08-02 00:00:00|           5|\n",
                        "|59cf5cb68bd420691...|bd96eed920dc1aa5f...| 2018-02-25 00:00:00|           5|\n",
                        "|37407068652762ec1...|9ce1af91f300b0213...| 2018-07-29 00:00:00|           5|\n",
                        "|8f4f0a9e03d6e7776...|c656e67f70a1830a6...| 2018-03-30 00:00:00|           5|\n",
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "w = Window.partitionBy(\"order_id\").orderBy(\"review_score\")\n",
                "\n",
                "df = (\n",
                "    review_df\n",
                "    .join(order_df, \"order_id\", \"inner\")\n",
                "    .join(customer_df, \"customer_id\", \"inner\")\n",
                "    .withColumn(\"rn\", F.row_number().over(w))\n",
                "    .where(\"rn = 1\")\n",
                "    .select(\n",
                "        \"customer_id\",\n",
                "        \"order_id\",\n",
                "        \"review_creation_date\",\n",
                "        \"review_score\",\n",
                "    )\n",
                ")\n",
                "\n",
                "df.orderBy(F.rand()).show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usando um dataframe de exemplo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|\n",
                        "+-----------+--------+--------------------+------------+\n",
                        "|          1|       1|                   1|           1|\n",
                        "|          1|       2|                   2|           2|\n",
                        "|          2|       1|                   1|           5|\n",
                        "|          2|       2|                   2|           1|\n",
                        "+-----------+--------+--------------------+------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "data = [(1, 1, 1, 1), (1, 2, 2, 2), (2, 1, 1, 5), (2, 2, 2, 1)]\n",
                "cols = [\"customer_id\", \"order_id\", \"review_creation_date\", \"review_score\"]\n",
                "df_exemplo = spark.createDataFrame(data, cols)\n",
                "\n",
                "df_exemplo.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|review_score_lag|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|          1|       1|                   1|           1|            NULL|\n",
                        "|          1|       2|                   2|           2|               1|\n",
                        "|          2|       1|                   1|           5|            NULL|\n",
                        "|          2|       2|                   2|           1|               5|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Vamos pegar o ultimo review com lag, para isso criar uma função para utilizar no dataset principal\n",
                "\n",
                "def create_lag_review(df):\n",
                "    w = (\n",
                "        Window\n",
                "        .partitionBy(\"customer_id\")\n",
                "        .orderBy(F.col(\"review_creation_date\").asc())\n",
                "    )\n",
                "\n",
                "    return (\n",
                "        df\n",
                "        .withColumn(\"review_score_lag\", F.lag(\"review_score\", 1).over(w))\n",
                "    )\n",
                "\n",
                "df_exemplo.transform(create_lag_review).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+---------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|review_score_ma|\n",
                        "+-----------+--------+--------------------+------------+---------------+\n",
                        "|          1|       1|                   1|           1|            1.0|\n",
                        "|          1|       2|                   2|           2|            1.5|\n",
                        "|          2|       1|                   1|           5|            5.0|\n",
                        "|          2|       2|                   2|           1|            3.0|\n",
                        "+-----------+--------+--------------------+------------+---------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Vamos calcular a media movel olhando para as ultimas linhas\n",
                "\n",
                "def create_ma_review(df):\n",
                "    w = (\n",
                "        Window\n",
                "        .partitionBy(\"customer_id\")\n",
                "        .rowsBetween(-1, 0)\n",
                "    )\n",
                "\n",
                "    return (\n",
                "        df\n",
                "        .withColumn(\"review_score_ma\", F.avg(\"review_score\").over(w))\n",
                "    )\n",
                "\n",
                "df_exemplo.transform(create_ma_review).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Aplicando no dataset de features e Salvando"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "26/02/07 01:22:15 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/processed/join_features_deduplicated.\n",
                        "java.io.FileNotFoundException: File data/processed/join_features_deduplicated does not exist\n",
                        "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
                        "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
                        "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
                        "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
                        "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
                        "\tat scala.Option.getOrElse(Option.scala:201)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
                        "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
                        "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
                        "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
                        "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
                        "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
                        "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
                        "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
                        "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
                        "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
                        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
                        "\tat scala.util.Try$.apply(Try.scala:217)\n",
                        "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
                        "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
                        "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
                        "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
                        "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
                        "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
                        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
                        "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
                        "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
                        "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
                        "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:305)\n",
                        "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
                        "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
                        "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
                        "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
                    ]
                },
                {
                    "ename": "AnalysisException",
                    "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/vho/alura/pyspark_alura/aulas/data/processed/join_features_deduplicated. SQLSTATE: 42K03",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m previous_path = \u001b[33m\"\u001b[39m\u001b[33mdata/processed/join_features_deduplicated\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m feature = (\n\u001b[32m      5\u001b[39m     df\n\u001b[32m      6\u001b[39m     .transform(create_lag_review)\n\u001b[32m      7\u001b[39m     .transform(create_ma_review)\n\u001b[32m      8\u001b[39m     .select(\u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreview_score_lag\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreview_score_ma\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m (\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     .join(feature, \u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m ).write.mode(\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m).parquet(save_path)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/alura/pyspark_alura/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/alura/pyspark_alura/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/alura/pyspark_alura/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
                        "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/vho/alura/pyspark_alura/aulas/data/processed/join_features_deduplicated. SQLSTATE: 42K03"
                    ]
                }
            ],
            "source": [
                "save_path = \"data/processed/final\"\n",
                "previous_path = \"data/processed/join_features_deduplicated\"\n",
                "\n",
                "feature = (\n",
                "    df\n",
                "    .transform(create_lag_review)\n",
                "    .transform(create_ma_review)\n",
                "    .select(\"order_id\", \"review_score_lag\", \"review_score_ma\")\n",
                ")\n",
                "\n",
                "(\n",
                "    spark.read.parquet(previous_path)\n",
                "    .join(feature, \"order_id\", \"left\")\n",
                ").write.mode(\"overwrite\").parquet(save_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
