{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5.2 - Features dos últimos review\n",
                "\n",
                "Neste notebook, exploraremos outras funções Window no PySpark. <br>\n",
                "Nesse processo vamos criar features baseadas nos reviews realizados por clientes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "26/02/07 00:30:02 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
                        "26/02/07 00:30:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "26/02/07 00:30:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "26/02/07 00:30:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
                        "26/02/07 00:30:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
                        "26/02/07 00:30:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.window import Window\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Join datasets\n",
                "\n",
                "Como vamos olhar os reviews do customer_id, precisamos juntas os datasets\n",
                "- reviews\n",
                "- orders\n",
                "- customers\n",
                "\n",
                "Dado que já temos conhecimento de como fazer join no PySpark, vamos utilizar o seguinte código"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- review_id: string (nullable = true)\n",
                        " |-- order_id: string (nullable = true)\n",
                        " |-- review_score: string (nullable = true)\n",
                        " |-- review_comment_title: string (nullable = true)\n",
                        " |-- review_comment_message: string (nullable = true)\n",
                        " |-- review_creation_date: string (nullable = true)\n",
                        " |-- review_answer_timestamp: string (nullable = true)\n",
                        "\n",
                        "root\n",
                        " |-- order_id: string (nullable = true)\n",
                        " |-- customer_id: string (nullable = true)\n",
                        " |-- order_status: string (nullable = true)\n",
                        " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
                        " |-- order_approved_at: timestamp (nullable = true)\n",
                        " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
                        " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
                        " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
                        "\n",
                        "root\n",
                        " |-- customer_id: string (nullable = true)\n",
                        " |-- customer_unique_id: string (nullable = true)\n",
                        " |-- customer_zip_code_prefix: integer (nullable = true)\n",
                        " |-- customer_city: string (nullable = true)\n",
                        " |-- customer_state: string (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "review_path = \"data/raw/olist_order_reviews_dataset.csv\"\n",
                "order_path = \"data/raw/olist_orders_dataset.csv\"\n",
                "customer_path = \"data/raw/olist_customers_dataset.csv\"\n",
                "\n",
                "review_df = spark.read.csv(review_path, header=True, inferSchema=True)\n",
                "order_df = spark.read.csv(order_path, header=True, inferSchema=True)\n",
                "customer_df = spark.read.csv(customer_path, header=True, inferSchema=True)\n",
                "\n",
                "review_df.printSchema()\n",
                "order_df.printSchema()\n",
                "customer_df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "|         customer_id|            order_id|review_creation_date|review_score|\n",
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "|5fb72531a5f56bbbf...|3eb7b3f39b5c39ce0...| 2018-01-20 00:00:00|           5|\n",
                        "|f8e4b4531b76c0efa...|9fe314dcda4135956...| 2017-05-30 00:00:00|           5|\n",
                        "|fe4a5493fd7197b7b...|16fac761b4906243e...| 2018-03-15 00:00:00|           4|\n",
                        "|af71c6566cc7ebbfe...|adc3c2a7a5283d29f...| 2018-08-16 00:00:00|           5|\n",
                        "|d55606cbf9e683ce2...|23b051786ba773bb6...| 2017-02-22 00:00:00|           5|\n",
                        "+--------------------+--------------------+--------------------+------------+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "w = Window.partitionBy(\"order_id\").orderBy(\"review_score\")\n",
                "\n",
                "df = (\n",
                "    review_df\n",
                "    .join(order_df, \"order_id\", \"inner\")\n",
                "    .join(customer_df, \"customer_id\", \"inner\")\n",
                "    .withColumn(\"rn\", F.row_number().over(w))\n",
                "    .where(\"rn = 1\")\n",
                "    .select(\n",
                "        \"customer_id\",\n",
                "        \"order_id\",\n",
                "        \"review_creation_date\",\n",
                "        \"review_score\",\n",
                "    )\n",
                ")\n",
                "\n",
                "df.orderBy(F.rand()).show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usando um dataframe de exemplo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|\n",
                        "+-----------+--------+--------------------+------------+\n",
                        "|          1|       1|                   1|           1|\n",
                        "|          1|       2|                   2|           2|\n",
                        "|          2|       1|                   1|           5|\n",
                        "|          2|       2|                   2|           1|\n",
                        "+-----------+--------+--------------------+------------+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "data = [(1, 1, 1, 1), (1, 2, 2, 2), (2, 1, 1, 5), (2, 2, 2, 1)]\n",
                "cols = [\"customer_id\", \"order_id\", \"review_creation_date\", \"review_score\"]\n",
                "df_exemplo = spark.createDataFrame(data, cols)\n",
                "\n",
                "df_exemplo.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|review_score_lag|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|          1|       1|                   1|           1|            NULL|\n",
                        "|          1|       2|                   2|           2|               1|\n",
                        "|          2|       1|                   1|           5|            NULL|\n",
                        "|          2|       2|                   2|           1|               5|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Vamos pegar o ultimo review com lag, para isso criar uma função para utilizar no dataset principal\n",
                "\n",
                "def create_lag_review(df):\n",
                "    w = (\n",
                "        Window\n",
                "        .partitionBy(\"customer_id\")\n",
                "        .orderBy(F.col(\"review_creation_date\").asc())\n",
                "    )\n",
                "\n",
                "    return (\n",
                "        df\n",
                "        .withColumn(\"review_score_lag\", F.lag(\"review_score\", 1).over(w))\n",
                "    )\n",
                "\n",
                "df_exemplo.transform(create_lag_review).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|customer_id|order_id|review_creation_date|review_score|review_score_lag|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "|          1|       1|                   1|           1|             1.0|\n",
                        "|          1|       2|                   2|           2|             1.5|\n",
                        "|          2|       1|                   1|           5|             5.0|\n",
                        "|          2|       2|                   2|           1|             3.0|\n",
                        "+-----------+--------+--------------------+------------+----------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Vamos calcular a media movel olhando para as ultimas linhas\n",
                "\n",
                "def create_ma_review(df):\n",
                "    w = (\n",
                "        Window\n",
                "        .partitionBy(\"customer_id\")\n",
                "        .rowsBetween(-1, 0)\n",
                "    )\n",
                "\n",
                "    return (\n",
                "        df\n",
                "        .withColumn(\"review_score_lag\", F.avg(\"review_score\").over(w))\n",
                "    )\n",
                "\n",
                "df_exemplo.transform(create_ma_review).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Aplicando no dataset de features e Salvando"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "path = \"data/processed/features_last_review\"\n",
                "\n",
                "(\n",
                "    df\n",
                "    .transform(create_lag_review)\n",
                "    .transform(create_ma_review)\n",
                "    .write\n",
                "    .mode(\"overwrite\")\n",
                "    .parquet(path)\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
