{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PySpark MLlib x Scikit-Learn\n",
                "\n",
                "| Feature | scikit-learn | PySpark ML |\n",
                "|---|---|---|\n",
                "| **Scale** | single machine | Distributes processing |\n",
                "| **Speed** | faster for smaller datasets | Slower for small datasets, but superior performance for massive data |\n",
                "| **API/Usability** | Integrates well with classic Python ecosystem (Pandas, Matplotlib) | The API is tailored for distributed systems, with a different approach (e.g., using VectorAssembler for feature consolidation). |\n",
                "| **Algorithms** | Offers a wider variety of algorithms and feature selection tools. | Includes core machine learning algorithms optimized for distributed processing|"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Algoritmos de Machine Learning Comuns\n",
                "Dentro do temos algumas opções de modelos\n",
                "Separei uma lista de modelos que funciona para Regressão e Classificação\n",
                "- Generalized Linear Regression\n",
                "    - Generalização da regressão linear ordinária. Podendo ser regressão linear, logistica poisson, ...\n",
                "- Decision Tree\n",
                "    - Árvores de Decisão (aprendizado supervisionado não paramétricos)\n",
                "\n",
                "- Random Forest\n",
                "    - Opera construindo uma infinidade de árvores de decisão\n",
                "\n",
                "- Gradient Boosting\n",
                "    - Conjunto de modelos de previsão fracos. Constrói o modelo em etapas, boosting, permitindo a otimização de uma função de perda diferenciável."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pipeline de Machine Learning (Classification)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "26/01/30 09:17:54 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
                        "26/01/30 09:17:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "26/01/30 09:17:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "26/01/30 09:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
                        "26/01/30 09:17:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, MinMaxScaler, Imputer\n",
                "from pyspark.ml import Pipeline\n",
                "from pyspark.ml.classification import GBTClassifier\n",
                "from pyspark.ml.regression import GBTRegressor\n",
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = spark.read.parquet(\"/Users/vho/alura/pyspark_course/aulas/olist.parquet\")\n",
                "\n",
                "train, test = df.randomSplit([0.7, 0.3], seed=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "82d015b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "categorical_cols = ['order_status', 'payment_primary_method']\n",
                "numeric_features = [\n",
                "    'order_days_between',\n",
                "    'order_count_item',\n",
                "    'order_sum_price',\n",
                "    'order_min_price',\n",
                "    'order_max_price',\n",
                "    'order_avg_price',\n",
                "    'order_sum_freight_value',\n",
                "    'order_min_freight_value',\n",
                "    'order_max_freight_value',\n",
                "    'order_avg_freight_value',\n",
                "    'payment_count',\n",
                "    'payment_min_value',\n",
                "    'payment_max_value',\n",
                "    'payment_avg_value',\n",
                "    'payment_sum_value',\n",
                "    'payment_max_installments',\n",
                "    'payment_primary_value',\n",
                "    'payment_primary_share'\n",
                "]\n",
                "assembler_inputs = numeric_features + [c + \"_vec\" for c in categorical_cols]\n",
                "\n",
                "imputer=Imputer(inputCols=['order_days_between'], outputCols=['order_days_between']).setStrategy('median')\n",
                "stringIndexer=StringIndexer(inputCols=categorical_cols, outputCols=[c + \"_index\" for c in categorical_cols])\n",
                "oneHotEncoder=OneHotEncoder(inputCols=[c + \"_index\" for c in categorical_cols], outputCols=[c + \"_vec\" for c in categorical_cols])\n",
                "vectorAssembler=VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_raw\")\n",
                "minMaxScaler=MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
                "\n",
                "stages = [\n",
                "    imputer,\n",
                "    stringIndexer,\n",
                "    oneHotEncoder,\n",
                "    vectorAssembler,\n",
                "    minMaxScaler\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "stages.append(GBTClassifier(featuresCol='features', labelCol='target', maxIter=10))\n",
                "\n",
                "pipeline = Pipeline(stages=stages)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "26/01/30 09:17:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "model = pipeline.fit(train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictions = model.transform(test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ae807f6c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------------------------------------+----------------------------------------+----------+------+\n",
                        "|rawPrediction                           |probability                             |prediction|target|\n",
                        "+----------------------------------------+----------------------------------------+----------+------+\n",
                        "|[-0.8289489528242735,0.8289489528242735]|[0.16004437995795773,0.8399556200420423]|1.0       |1     |\n",
                        "|[-0.8523026280645966,0.8523026280645966]|[0.15386474881524081,0.8461352511847592]|1.0       |1     |\n",
                        "|[-0.865401130581826,0.865401130581826]  |[0.15048499051308914,0.8495150094869108]|1.0       |1     |\n",
                        "|[-0.8542145655766816,0.8542145655766816]|[0.15336757569579207,0.8466324243042079]|1.0       |1     |\n",
                        "|[0.1489848405117163,-0.1489848405117163]|[0.5739461136055757,0.4260538863944243] |0.0       |1     |\n",
                        "+----------------------------------------+----------------------------------------+----------+------+\n",
                        "only showing top 5 rows\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "26/01/30 09:18:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
                    ]
                }
            ],
            "source": [
                "predictions.select(\"rawPrediction\", \"probability\", \"prediction\", \"target\").show(5, truncate=False)\n",
                "# Raw Prediction: Output da arvore de decisão, simetrico\n",
                "# Probability: Probabilidade de cada classe (sigmoid) sum=1\n",
                "# Prediction: Classe prevista (corte 0.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Area Under ROC: 0.6960817953946045\n",
                        "Area Under PR: 0.8734206204473458\n",
                        "Accuracy: 0.8167445455503636\n",
                        "F1: 0.7770910154424593\n",
                        "Precision: 0.7969131250592675\n",
                        "Recall: 0.8167445455503637\n"
                    ]
                }
            ],
            "source": [
                "evaluator_binary = BinaryClassificationEvaluator(labelCol=\"target\")\n",
                "evaluator_multiclass = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n",
                "\n",
                "# Usando probability\n",
                "print(f\"Area Under ROC: {evaluator_binary.evaluate(predictions, {evaluator_binary.metricName: 'areaUnderROC'})}\")\n",
                "print(f\"Area Under PR: {evaluator_binary.evaluate(predictions, {evaluator_binary.metricName: 'areaUnderPR'})}\")\n",
                "\n",
                "# Usando prediction\n",
                "print(f\"Accuracy: {evaluator_multiclass.evaluate(predictions, {evaluator_multiclass.metricName: 'accuracy'})}\")\n",
                "print(f\"F1: {evaluator_multiclass.evaluate(predictions, {evaluator_multiclass.metricName: 'f1'})}\")\n",
                "print(f\"Precision: {evaluator_multiclass.evaluate(predictions, {evaluator_multiclass.metricName: 'weightedPrecision'})}\")\n",
                "print(f\"Recall: {evaluator_multiclass.evaluate(predictions, {evaluator_multiclass.metricName: 'weightedRecall'})}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pipeline de Machine Learning (Regression)\n",
                "\n",
                "Agora que já explicamos classificação, vamos para regressão. Dessa vez coloque o GBTRegressor no pipeline fazendo o fit e prediction na mesma celula."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_reg = train.withColumn(\"review_score\", F.col(\"review_score\").cast(\"double\"))\n",
                "test_reg = test.withColumn(\"review_score\", F.col(\"review_score\").cast(\"double\"))\n",
                "\n",
                "stages = [\n",
                "    imputer,\n",
                "    stringIndexer,\n",
                "    oneHotEncoder,\n",
                "    vectorAssembler,\n",
                "    minMaxScaler\n",
                "]\n",
                "\n",
                "gbt_reg = GBTRegressor(featuresCol='features', labelCol='review_score')\n",
                "stages.append(gbt_reg)\n",
                "\n",
                "pipeline_reg = Pipeline(stages=stages)\n",
                "\n",
                "model_reg = pipeline_reg.fit(train_reg)\n",
                "predictions_reg = model_reg.transform(test_reg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE: 1.1659161430976759\n",
                        "R2: 0.17153392787111132\n"
                    ]
                }
            ],
            "source": [
                "evaluator_reg = RegressionEvaluator(labelCol=\"review_score\", predictionCol=\"prediction\")\n",
                "\n",
                "rmse = evaluator_reg.evaluate(predictions_reg, {evaluator_reg.metricName: \"rmse\"})\n",
                "r2 = evaluator_reg.evaluate(predictions_reg, {evaluator_reg.metricName: \"r2\"})\n",
                "\n",
                "print(f\"RMSE: {rmse}\")\n",
                "print(f\"R2: {r2}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
