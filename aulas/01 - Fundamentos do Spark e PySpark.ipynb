{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZBlzsJt0cWC"
   },
   "source": [
    "# Fundamentos do Spark e PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpA9fHoU0rm5"
   },
   "source": [
    "## Verificando os pré-requisitos  para rodar o PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1770035733753,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "ql2lm4wgz_hM",
    "outputId": "0c1068cf-5791-4c5a-9b40-e2edfc4a16b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.10\" 2026-01-20\n",
      "OpenJDK Runtime Environment Homebrew (build 21.0.10)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 21.0.10, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# \"PySpark requires Java 17 or later\"\n",
    "# 1. Verificando a versão do Java instalada\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2843,
     "status": "ok",
     "timestamp": 1770035736610,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "WSkUAMEJbixW",
    "outputId": "1ff9e81b-5b1b-4d78-efe6-05ff11b07144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.11 environment at: /Users/vho/alura/pyspark_alura/.venv\u001b[0m\n",
      "pyspark                   4.1.1\n"
     ]
    }
   ],
   "source": [
    "# Verificando se o pyspark está instalado\n",
    "!uv pip list | grep pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skdAvAWC3QKK"
   },
   "source": [
    "## Inicializando o SparkSession\n",
    "O SparkSession atua como o ponto de entrada unificado para as funcionalidades do Apache Spark. Sua principais funções são:\n",
    "1. Permitir a interação com as APIs de DataFrames e Datasets\n",
    "2. Gerenciando desde as configurações de hardware e variáveis de ambiente até a execução de consultas SQL\n",
    "3. Coordenar a distribuição de tarefas entre os nós de um cluster\n",
    "\n",
    "\n",
    "<img src='https://spark.apache.org/docs/latest/img/cluster-overview.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vMXov_Pg0xqJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/03 22:14:46 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
      "26/02/03 22:14:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/03 22:14:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770035753561,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "HNUZirKR66QP",
    "outputId": "d4ff2184-8724-43e1-a334-ca33001854a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Só para mostrar que estamos rodando o spark localmente - não em um conjunto de computadores (cluster)\n",
    "spark.sparkContext.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHu1YPIh7dH7"
   },
   "source": [
    "## Criando um DataFrame de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14939,
     "status": "ok",
     "timestamp": 1770035768501,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "72d05490",
    "outputId": "ed2e34d5-06c8-49f7-cfbd-f32a1edcd43a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nome: string, nascimento: string, nota: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"João\", \"20/05/1998\", 10.0),\n",
    "    (\"Ana\", \"19/05/2000\", 9.5),\n",
    "]\n",
    "columns = [\"nome\",\"nascimento\",\"nota\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----+\n",
      "|nome|nascimento|nota|\n",
      "+----+----------+----+\n",
      "|João|20/05/1998|10.0|\n",
      "| Ana|19/05/2000| 9.5|\n",
      "+----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1770035768551,
     "user": {
      "displayName": "Vitor Mello",
      "userId": "01419285889285209692"
     },
     "user_tz": 180
    },
    "id": "aEDp_Ss_iXKy",
    "outputId": "e740f43f-96c1-4e6f-9a46-cbf270f9df8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- nascimento: string (nullable = true)\n",
      " |-- nota: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkK7AqzX8iI7"
   },
   "source": [
    "## Explicação de Lazy Evaluation\n",
    "\n",
    "**Resumo**: As transformações são executadas somente quando é necessario (uma action é executada), dessa forma (1) o Spark valida o nosso codigo sem a necessidade de executar todas a nossa query e (2) o Spark otimiza a query selecionando formas otimizadas de execução\n",
    "\n",
    "Conceitos importantes para o entendimento:\n",
    "- Transformation: metodo que retornar um DataFrame (ex: `.select()`)\n",
    "- Action: metodo que retornar um valor (ex: `.count()` ou `.show()`)\n",
    "- Catalyst Optimizer: mecanismo do spark que otimiza todas as transformaçõesque precisam ser executadas\n",
    "\n",
    "<img src='https://www.databricks.com/wp-content/uploads/2018/05/Catalyst-Optimizer-diagram.png'>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOct15XGU8n57bVPo1bV2Hg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
