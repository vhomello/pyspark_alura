{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5.1 - Window Functions - Row Number\n",
                "\n",
                "Neste notebook, vamos explorar as `Window Functions` (Funções de Janela) no PySpark e como utilizar row number para deduplicar dados "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "26/02/06 23:08:18 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
                        "26/02/06 23:08:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "26/02/06 23:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "26/02/06 23:08:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
                        "26/02/06 23:08:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "import pyspark.sql.functions as F\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para usar Window functions, precisamos importar Window do módulo `pyspark.sql.window`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.window import Window"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Deduplicando o dataset criado na ultima aula"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "root\n",
                        " |-- order_id: string (nullable = true)\n",
                        " |-- review_id: string (nullable = true)\n",
                        " |-- review_score: integer (nullable = true)\n",
                        " |-- target: integer (nullable = true)\n",
                        " |-- count_payment_value: long (nullable = true)\n",
                        " |-- min_payment_value: double (nullable = true)\n",
                        " |-- max_payment_value: double (nullable = true)\n",
                        " |-- avg_payment_value: double (nullable = true)\n",
                        " |-- sum_payment_value: double (nullable = true)\n",
                        " |-- stddev_payment_value: double (nullable = true)\n",
                        " |-- purchase_month: integer (nullable = true)\n",
                        " |-- purchase_week: integer (nullable = true)\n",
                        " |-- purchase_day_week: integer (nullable = true)\n",
                        " |-- days_between: integer (nullable = true)\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "path = \"data/processed/join_features\"\n",
                "\n",
                "df = spark.read.parquet(path)\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+--------------------+-----+\n",
                        "|            order_id|count|\n",
                        "+--------------------+-----+\n",
                        "|df56136b8031ecd28...|    3|\n",
                        "|8e17072ec97ce29f0...|    3|\n",
                        "|03c939fd7fd3b38f8...|    3|\n",
                        "|c88b1d1b157a9999c...|    3|\n",
                        "|33f1e992ba3e439bf...|    2|\n",
                        "+--------------------+-----+\n",
                        "only showing top 5 rows\n"
                    ]
                }
            ],
            "source": [
                "# Verificando que o DataFrame está com o id duplicado\n",
                "\n",
                "(\n",
                "    df\n",
                "    .groupBy(\"order_id\")\n",
                "    .count()\n",
                "    .orderBy(F.col(\"count\").desc())\n",
                ").show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+-----+\n",
                        "| id|score|\n",
                        "+---+-----+\n",
                        "|  1|    1|\n",
                        "|  1|    2|\n",
                        "|  2|    5|\n",
                        "|  2|    1|\n",
                        "+---+-----+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Vamos pegar a pior nota de cada order_id\n",
                "# Antes de aplicar no dataset final vamos utilizar um exemplo\n",
                "\n",
                "data = [(1, 1), (1, 2), (2, 5), (2, 1)]\n",
                "df_exemplo = spark.createDataFrame(data, [\"id\", \"score\"])\n",
                "df_exemplo.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+-----+---+\n",
                        "| id|score| rn|\n",
                        "+---+-----+---+\n",
                        "|  1|    1|  1|\n",
                        "|  2|    1|  2|\n",
                        "|  1|    2|  3|\n",
                        "|  2|    5|  4|\n",
                        "+---+-----+---+\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "26/02/06 23:15:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
                        "26/02/06 23:15:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
                        "26/02/06 23:15:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
                    ]
                }
            ],
            "source": [
                "# Vamos ver como funciona o row_number ordenado pelo score\n",
                "(\n",
                "    df_exemplo\n",
                "    .withColumn(\"rn\", F.row_number().over(Window.orderBy(\"score\")))\n",
                ").show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+---+-----+---+\n",
                        "| id|score| rn|\n",
                        "+---+-----+---+\n",
                        "|  1|    1|  1|\n",
                        "|  1|    2|  2|\n",
                        "|  2|    1|  1|\n",
                        "|  2|    5|  2|\n",
                        "+---+-----+---+\n",
                        "\n",
                        "+---+-----+---+\n",
                        "| id|score| rn|\n",
                        "+---+-----+---+\n",
                        "|  1|    1|  1|\n",
                        "|  2|    1|  1|\n",
                        "+---+-----+---+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Na ultima celula misturou os ids, para isso vamos utilizar partitionBy\n",
                "\n",
                "(\n",
                "    df_exemplo\n",
                "    .withColumn(\"rn\", F.row_number().over(Window.partitionBy(\"id\").orderBy(\"score\")))\n",
                ").show()\n",
                "\n",
                "# Agora para finalizar a deduplicação vamos filtrar rn = 1 \n",
                "\n",
                "(\n",
                "    df_exemplo\n",
                "    .withColumn(\"rn\", F.row_number().over(Window.partitionBy(\"id\").orderBy(\"score\")))\n",
                "    .where(F.col(\"rn\")==1)\n",
                ").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Aplicando no dataset de features e Salvando"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path = \"data/processed/join_features_deduplicated\"\n",
                "\n",
                "w = Window.partitionBy(\"order_id\").orderBy(\"review_score\")\n",
                "\n",
                "(\n",
                "    df\n",
                "    .withColumn(\"rn\", F.row_number().over(w))\n",
                "    .where(F.col(\"rn\") == 1)\n",
                "    .write\n",
                "    .mode(\"overwrite\")\n",
                "    .parquet(path)\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
