{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa036f1",
   "metadata": {},
   "source": [
    "# 3.1 - Criando o nosso Target "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09569054",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10508d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/05 21:18:54 WARN Utils: Your hostname, MacBook-Air-de-Vitor.local, resolves to a loopback address: 127.0.0.1; using 192.168.3.49 instead (on interface en0)\n",
      "26/02/05 21:18:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/05 21:18:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcaedd1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb75e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|           review_id|            order_id|review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|e22aad953e113cdcc...|8b2c286fa36b36c81...|           4|                NULL|               Nota 10| 2017-09-19 00:00:00|    2017-09-20 01:07:14|\n",
      "|5ddb195ab2206a456...|18937b40506fdcbd3...|           4|                NULL|                  NULL| 2017-05-30 00:00:00|    2017-07-06 18:36:02|\n",
      "|4d3c61768eb47216e...|cb4a79c1e6c9ae443...|           1|                NULL|  Absurdo! Venderam...| 2018-01-05 00:00:00|    2018-01-15 11:04:49|\n",
      "|5ef9614ed02a28935...|2e1934467537a71d1...|           3|                NULL|                  NULL| 2018-08-01 00:00:00|    2018-08-02 12:00:50|\n",
      "|f62709ef754361d3e...|c812c0f1cdbb15686...|           5|                NULL|       recebi no prazo| 2017-03-14 00:00:00|    2017-03-17 08:16:49|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Carregando o dataset criado na ultima aula\n",
    "\n",
    "path = 'data/processed/olist_order_reviews_dataset'\n",
    "\n",
    "df = spark.read.parquet(path)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34185493",
   "metadata": {},
   "source": [
    "## Select e Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06001245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|review_score|\n",
      "+------------+\n",
      "|4           |\n",
      "|4           |\n",
      "|1           |\n",
      "|3           |\n",
      "|5           |\n",
      "+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Vamos Selecionar a coluna de review\n",
    "\n",
    "df.select(\n",
    "    \"review_score\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4afdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|     review_score|\n",
      "+-------+-----------------+\n",
      "|  count|            99225|\n",
      "|   mean|4.086379440665155|\n",
      "| stddev|1.347634781943274|\n",
      "|    min|                0|\n",
      "|    max|                5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos fazer calcular algunas estatisticas dessa coluna\n",
    "\n",
    "df.select(\"review_score\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802ab6e",
   "metadata": {},
   "source": [
    "## Criando o nosso target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9913fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma pratica comum é importar o modulo functions como F\n",
    "# Evitando sobrescrever functions do python como min, max, ...\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1abf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+--------------+\n",
      "|           review_id|review_score|bom_review|bom_review_int|\n",
      "+--------------------+------------+----------+--------------+\n",
      "|e22aad953e113cdcc...|           4|      true|             1|\n",
      "|5ddb195ab2206a456...|           4|      true|             1|\n",
      "|4d3c61768eb47216e...|           1|     false|             0|\n",
      "|5ef9614ed02a28935...|           3|     false|             0|\n",
      "|f62709ef754361d3e...|           5|      true|             1|\n",
      "+--------------------+------------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Criando o target binario (1 ou 0)\n",
    "\n",
    "(\n",
    "    df\n",
    "    .select(\"review_id\", \"review_score\")\n",
    "    .withColumn(\"bom_review\", F.col(\"review_score\") >= 4)\n",
    "    .withColumn(\"bom_review_int\", F.col(\"bom_review\").cast(\"int\"))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d106c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+\n",
      "|           review_id|review_score|target|\n",
      "+--------------------+------------+------+\n",
      "|e22aad953e113cdcc...|           4|     1|\n",
      "|5ddb195ab2206a456...|           4|     1|\n",
      "|4d3c61768eb47216e...|           1|     0|\n",
      "|5ef9614ed02a28935...|           3|     0|\n",
      "|f62709ef754361d3e...|           5|     1|\n",
      "+--------------------+------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Podemos fazer isso de uma forma mais parecida com o SQL\n",
    "\n",
    "df.select(\n",
    "    F.col(\"review_id\"),\n",
    "    F.col(\"review_score\"),\n",
    "    (F.col(\"review_score\") >= 4).cast(\"int\").alias(\"target\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9796e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/processed/target\"\n",
    "target_logic = (F.col(\"review_score\") >= 4).cast(\"int\").alias(\"target\")\n",
    "\n",
    "target_df = df.select(\n",
    "    F.col(\"review_id\"),\n",
    "    F.col(\"order_id\"),\n",
    "    F.col(\"review_score\"),\n",
    "    target_logic\n",
    ").write.mode(\"overwrite\").parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a0132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36mdata/processed/target\u001b[0m\n",
      "├── _SUCCESS\n",
      "├── part-00000-baab187e-0823-4328-86d9-495b99f71c2e-c000.snappy.parquet\n",
      "├── part-00001-baab187e-0823-4328-86d9-495b99f71c2e-c000.snappy.parquet\n",
      "├── part-00002-baab187e-0823-4328-86d9-495b99f71c2e-c000.snappy.parquet\n",
      "└── part-00003-baab187e-0823-4328-86d9-495b99f71c2e-c000.snappy.parquet\n",
      "\n",
      "1 directory, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree data/processed/target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
